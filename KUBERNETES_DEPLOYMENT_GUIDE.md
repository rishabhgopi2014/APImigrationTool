# ðŸš€ Kubernetes Deployment Guide

## Overview

This guide shows you how to deploy Gloo Gateway configurations generated by the API Migration Orchestrator to a **real Kubernetes cluster**.

---

## Prerequisites

### âœ… **Required:**

1. **Kubernetes Cluster** (v1.20+)
   - Minikube, GKE, EKS, AKS, or on-prem
   - kubectl configured and connected

2. **Gloo Gateway Installed**
   - Gloo Edge or Gloo Enterprise
   - Version 1.14+ recommended

3. **Generated Configs**
   - YAML files from the migration tool
   - VirtualService, Upstream, AuthConfig, RateLimitConfig

4. **Network Access**
   - Connectivity to legacy backend systems
   - DNS resolution configured

---

## Step 1: Verify Gloo Installation

### Check Gloo Gateway is Running

```bash
# Check Gloo namespace
kubectl get ns gloo-system

# Check Gloo pods
kubectl get pods -n gloo-system

# Expected output:
# NAME                             READY   STATUS    RESTARTS   AGE
# discovery-xxxxxx                 1/1     Running   0          5d
# gateway-proxy-xxxxxx             1/1     Running   0          5d
# gloo-xxxxxx                      1/1     Running   0          5d
```

### Verify Gateway Proxy Service

```bash
# Get gateway service
kubectl get svc -n gloo-system gateway-proxy

# Expected output:
# NAME            TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)
# gateway-proxy   LoadBalancer   10.96.xxx.xxx   <pending/IP>    80:31234/TCP,443:31235/TCP
```

---

## Step 2: Prepare Backend Connectivity

### Configure DNS or Service Discovery

**Option A: External Backend (Legacy APIC)**
```yaml
# Create upstream pointing to legacy system
apiVersion: gloo.solo.io/v1
kind: Upstream
metadata:
  name: legacy-apic-upstream
  namespace: gloo-system
spec:
  static:
    hosts:
      - addr: legacy-apic.yourcompany.com
        port: 8443
  sslConfig:
    sni: legacy-apic.yourcompany.com
```

**Option B: In-Cluster Service**
```yaml
# If backend is in K8s
apiVersion: gloo.solo.io/v1
kind: Upstream
metadata:
  name: backend-service-upstream
  namespace: gloo-system
spec:
  kube:
    serviceName: backend-api
    serviceNamespace: default
    servicePort: 8080
```

---

## Step 3: Deploy Generated Configs

### Method 1: Single API Deployment

```bash
# Navigate to generated configs directory
cd generated-configs/customer-preferences-api

# Review files
ls -la
# virtualservice.yaml
# upstream.yaml
# authconfig.yaml (if auth required)
# ratelimitconfig.yaml (if rate limiting required)

# Apply in order:

# 1. Create Upstream (backend target)
kubectl apply -f upstream.yaml

# 2. Create AuthConfig (if needed)
kubectl apply -f authconfig.yaml

# 3. Create RateLimitConfig (if needed)
kubectl apply -f ratelimitconfig.yaml

# 4. Create VirtualService (routing)
kubectl apply -f virtualservice.yaml

# Verify
kubectl get virtualservices -n gloo-system
kubectl get upstreams -n gloo-system
```

### Method 2: Bulk Deployment

```bash
# Apply all configs for one API
kubectl apply -f generated-configs/customer-preferences-api/

# Apply all APIs at once (careful!)
kubectl apply -f generated-configs/ -R
```

---

## Step 4: Verify Deployment

### Check Resource Status

```bash
# Check VirtualServices
kubectl get vs -n gloo-system

# Describe for details
kubectl describe vs customer-preferences-api-vs -n gloo-system

# Check Upstreams
kubectl get upstream -n gloo-system

# Check if upstream is discovered
kubectl get upstream customer-preferences-api-upstream -n gloo-system -o yaml
```

### Verify Gloo Accepted Configuration

```bash
# Check VirtualService status
kubectl get vs customer-preferences-api-vs -n gloo-system -o jsonpath='{.status.state}'
# Output should be: Accepted

# Check errors (if any)
kubectl get vs customer-preferences-api-vs -n gloo-system -o jsonpath='{.status.reason}'
```

---

## Step 5: Test the API

### Get Gateway URL

```bash
# For LoadBalancer
export GATEWAY_URL=$(kubectl get svc -n gloo-system gateway-proxy -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

# For NodePort (Minikube, etc.)
export GATEWAY_URL=$(minikube ip):$(kubectl get svc -n gloo-system gateway-proxy -o jsonpath='{.spec.ports[?(@.name=="http")].nodePort}')

echo "Gateway URL: $GATEWAY_URL"
```

### Test with curl

```bash
# Test basic connectivity
curl -v http://$GATEWAY_URL/customer/v1/health

# Test with authentication (if OAuth)
curl -H "Authorization: Bearer YOUR_TOKEN" \
     http://$GATEWAY_URL/customer/v1/preferences

# Test rate limiting
for i in {1..100}; do
  curl -s http://$GATEWAY_URL/customer/v1/test
done
# Should see 429 Too Many Requests after limit
```

---

## Step 6: Traffic Mirroring (Safe Testing)

### Configure Mirroring to Gloo

```yaml
# Add to your existing VirtualService
apiVersion: gateway.solo.io/v1
kind: VirtualService
metadata:
  name: customer-preferences-api-vs
  namespace: gloo-system
spec:
  virtualHost:
    domains:
      - 'api.yourcompany.com'
    routes:
      - matchers:
          - prefix: /customer/v1
        routeAction:
          single:
            upstream:
              name: legacy-apic-upstream  # Primary (100%)
              namespace: gloo-system
          # Mirror traffic to new Gloo upstream
          options:
            shadowing:
              upstream:
                name: customer-preferences-api-upstream
                namespace: gloo-system
              percentage: 100  # Mirror 100% of traffic
```

**Apply the updated config:**
```bash
kubectl apply -f virtualservice-with-mirror.yaml
```

**Monitor mirrored traffic:**
```bash
# Check Gloo logs
kubectl logs -n gloo-system deployment/gateway-proxy -f

# Check backend logs (if accessible)
kubectl logs -n default deployment/customer-api -f
```

---

## Step 7: Canary Rollout (Gradual Traffic Shift)

### Phase 1: 5% Traffic to Gloo

```yaml
apiVersion: gateway.solo.io/v1
kind: VirtualService
metadata:
  name: customer-preferences-api-vs
  namespace: gloo-system
spec:
  virtualHost:
    domains:
      - 'api.yourcompany.com'
    routes:
      - matchers:
          - prefix: /customer/v1
        routeAction:
          multi:
            destinations:
              - weight: 95
                destination:
                  upstream:
                    name: legacy-apic-upstream
                    namespace: gloo-system
              - weight: 5  # Start with 5%
                destination:
                  upstream:
                    name: customer-preferences-api-upstream
                    namespace: gloo-system
```

### Phase 2: Increase Gradually

```bash
# Update weights: 10%, 25%, 50%, 75%, 100%
# After monitoring each phase for 2-4 hours

# Monitor for errors
kubectl logs -n gloo-system deployment/gateway-proxy | grep -i error

# Check metrics (if Prometheus installed)
kubectl port-forward -n gloo-system deployment/gateway-proxy 9091:9091
# Visit http://localhost:9091/metrics
```

---

## Step 8: Complete Migration (100% Gloo)

### Final Configuration

```yaml
apiVersion: gateway.solo.io/v1
kind: VirtualService
metadata:
  name: customer-preferences-api-vs
  namespace: gloo-system
spec:
  virtualHost:
    domains:
      - 'api.yourcompany.com'
    routes:
      - matchers:
          - prefix: /customer/v1
        routeAction:
          single:
            upstream:
              name: customer-preferences-api-upstream  # 100% to Gloo
              namespace: gloo-system
```

### Cleanup Legacy Upstream

```bash
# Once stable for 1 week, remove legacy upstream
kubectl delete upstream legacy-apic-upstream -n gloo-system
```

---

## Rollback Procedures

### Emergency Rollback (< 5 seconds)

```bash
# Revert to 100% legacy
kubectl apply -f virtualservice-legacy-only.yaml

# Or edit live
kubectl edit vs customer-preferences-api-vs -n gloo-system
# Change weight: 100 for legacy, 0 for Gloo
```

### Complete Removal

```bash
# Delete all Gloo resources for this API
kubectl delete vs customer-preferences-api-vs -n gloo-system
kubectl delete upstream customer-preferences-api-upstream -n gloo-system
kubectl delete authconfig customer-preferences-api-auth -n gloo-system
kubectl delete ratelimitconfig customer-preferences-api-rl -n gloo-system
```

---

## Monitoring and Observability

### Prometheus Metrics

```bash
# Port-forward to gateway
kubectl port-forward -n gloo-system deployment/gateway-proxy 9091:9091

# Key metrics:
# - envoy_http_downstream_rq_total (total requests)
# - envoy_http_downstream_rq_xx (status codes)
# - envoy_http_downstream_rq_time (latency)
```

### Access Logs

```bash
# Enable access logging (if not already enabled)
kubectl edit settings -n gloo-system default

# Add:
spec:
  gateway:
    options:
      accessLoggingService:
        accessLog:
          - fileSink:
              path: /dev/stdout
              stringFormat: |
                [%START_TIME%] "%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%" %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% "%REQ(X-FORWARDED-FOR)%" "%REQ(USER-AGENT)%" "%REQ(X-REQUEST-ID)%"

# View logs
kubectl logs -n gloo-system deployment/gateway-proxy -f
```

---

## Troubleshooting

### VirtualService Not Accepted

```bash
# Check status
kubectl get vs -n gloo-system customer-preferences-api-vs -o yaml

# Common issues:
# 1. Invalid upstream reference
# 2. Domain conflicts with other VS
# 3. Invalid route matcher syntax
# 4. Missing AuthConfig/RateLimitConfig
```

### Upstream Not Discovered

```bash
# Check upstream status
kubectl get upstream -n gloo-system customer-preferences-api-upstream -o yaml

# Verify connectivity
kubectl run -it --rm debug --image=curlimages/curl --restart=Never -- \
  curl -v http://customer-api.default.svc.cluster.local:8080/health
```

### 503 Service Unavailable

```bash
# Check if backend is healthy
kubectl get endpoints -A | grep customer-api

# Check Gloo logs
kubectl logs -n gloo-system deployment/gloo -f | grep -i error

# Check gateway logs
kubectl logs -n gloo-system deployment/gateway-proxy -f
```

### Rate Limiting Not Working

```bash
# Verify RateLimitConfig exists
kubectl get ratelimitconfig -n gloo-system

# Check if rate limit server is running
kubectl get pods -n gloo-system | grep rate-limit

# Check envoy rate limit stats
kubectl exec -n gloo-system deployment/gateway-proxy -- \
  curl -s localhost:19000/stats | grep ratelimit
```

---

## Best Practices

### 1. Namespace Organization
```bash
# Keep all Gloo resources in gloo-system
# Keep backend services in their own namespaces
```

### 2. Resource Naming
```bash
# Use consistent naming: <api-name>-<resource-type>
# customer-preferences-api-vs
# customer-preferences-api-upstream
# customer-preferences-api-auth
```

###  3. Version Control
```bash
# Store all YAML in Git
git add generated-configs/
git commit -m "Add Gloo configs for customer-preferences-api"
git push
```

### 4. Testing in Staging First
```bash
# Deploy to staging cluster first
kubectl --context=staging apply -f virtualservice.yaml

# Test thoroughly
# Then deploy to production
kubectl --context=production apply -f virtualservice.yaml
```

### 5. Gradual Rollout
```bash
# Never go 0% â†’ 100% in one step
# Always: 0 â†’ 5 â†’ 10 â†’ 25 â†’ 50 â†’ 75 â†’ 100
# Monitor for 2-4 hours at each step
```

---

## Quick Reference

### Essential Commands

```bash
# Deploy API
kubectl apply -f generated-configs/my-api/

# Check status
kubectl get vs,upstream,authconfig -n gloo-system

# View logs
kubectl logs -n gloo-system deployment/gateway-proxy -f

# Test API
curl http://$GATEWAY_URL/api/v1/endpoint

# Rollback
kubectl apply -f virtualservice-backup.yaml

# Delete API
kubectl delete -f generated-configs/my-api/
```

---

## Next Steps

1. âœ… Deploy 1-2 low-risk APIs first
2. âœ… Monitor for 1 week
3. âœ… Document any issues/workarounds
4. âœ… Scale to  more APIs
5. âœ… Automate deployment with CI/CD

**You're ready to deploy!** ðŸš€
